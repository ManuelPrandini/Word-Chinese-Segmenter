{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bies_format(line):\n",
    "    '''\n",
    "    Method that create the corrispettive bies format of a string\n",
    "    @param line = the input string\n",
    "    @return the bias format of the input line\n",
    "    '''\n",
    "    i = 0\n",
    "    bies = \"\"\n",
    "    line = line.replace('\\n',\"\")\n",
    "    for index in range(len(line)):\n",
    "        if(index < len(line)-1):\n",
    "            if(line[index] == ' '):\n",
    "                i = 0\n",
    "            elif(i == 0 and line[index+1] == ' '):\n",
    "                i = 0\n",
    "                bies+='s'\n",
    "            elif(i == 0 and line[index+1] != ' '):\n",
    "                bies+='b'\n",
    "                i+=1\n",
    "            elif(i > 0 and line[index+1] != ' '):\n",
    "                bies+='i'\n",
    "                i+=1\n",
    "            else :\n",
    "                bies+='e'\n",
    "                i=0\n",
    "        else:\n",
    "            if(i==0):\n",
    "                bies+='s'\n",
    "            elif(i>0):\n",
    "                bies+='e'\n",
    "    return bies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bies_lines(path):\n",
    "    '''\n",
    "    method used to create an array that contains the bies format lines\n",
    "    of a specific file passed through the path\n",
    "    @param path = the pathfile of the file to convert\n",
    "    @return an array with bies lines\n",
    "    '''\n",
    "    bies_lines = []\n",
    "    with open(path,'r',encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            bies = create_bies_format(line)\n",
    "            bies_lines.append(bies)\n",
    "        f.close()\n",
    "    return bies_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_spaces_file(path):\n",
    "    '''\n",
    "    Method used to create an array that contains the lines without spaces\n",
    "    @param path input file\n",
    "    @return an array with sentences without space\n",
    "    '''\n",
    "    result = []\n",
    "    with open(path,'r',encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            line = line.replace(\" \",\"\")\n",
    "            result.append(line)\n",
    "        f.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(path,bies_lines):\n",
    "    '''\n",
    "    Method used to create a file either for label and tensorInput \n",
    "    @param path = the path where to save the file\n",
    "    @param bies_lines = the array containing the bies format sentences\n",
    "    '''\n",
    "    with open(path,'w') as f:\n",
    "        for bies in bies_lines:\n",
    "            f.write(bies+'\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINISCO I PATH DEI FILES\n",
    "path_training_input = \"../resources/icwb2-data/training/msr_training.utf8\"\n",
    "path_training_label = \"../resources/icwb2-data/training/bies_msr_training.utf8\"\n",
    "path_training_tensor = \"../resources/icwb2-data/training/tensor_msr_training.utf8\"\n",
    "\n",
    "\n",
    "#CREO GLI ARRAY DA INSERIRE NEI FILES DA CREARE\n",
    "bies_result = create_bies_lines(path_training_input)\n",
    "tensor_result = delete_spaces_file(path_training_input)\n",
    "\n",
    "#CREO I RISPETTIVI FILES\n",
    "create_file(path_training_label,bies_result)\n",
    "create_file(path_training_tensor,tensor_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'b', 'e', 's', 's', 'b', 'e', 's', 's', 's', 'b', 'i', 'e', 's', 's', 's', 's', 's', 's', 'b', 'e', 's', 's', 'b', 'i', 'i', 'e', 's', 'b', 'i', 'e', 's', 's', 'b', 'e', 's', 'b', 'i', 'i', 'e', 's', 's', 's', 's', 'b', 'e', 's', 's']\n"
     ]
    }
   ],
   "source": [
    "def split_into_ngrams(sentence: str, n : int):\n",
    "    \"\"\"\n",
    "    Split a sentence in array of ngrams\n",
    "    :param sentence Sentence as str\n",
    "    :return an array of ngrams\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    for i in range(len(sentence)-(n-1)):\n",
    "        ngram = sentence[i:i+n]\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "print(split_into_ngrams(bies_result[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(sentences,n):\n",
    "    '''\n",
    "    :param sentences List of sentences used to build the vocab\n",
    "    :return vocab Dictionary that has as key the ngram and as a value the index\n",
    "    '''\n",
    "    vocab = {0:0,\"UNK\": 1}\n",
    "    for sentence in sentences:\n",
    "        bigrams = split_into_ngrams(sentence,n)\n",
    "        for bigram in bigrams:\n",
    "            if bigram not in vocab:\n",
    "                vocab[bigram] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CREATE THE UNIGRAMS VOCAB FOR TENSOR \n",
    "tensor_unigrams = make_vocab(tensor_result,1)\n",
    "#CREATE THE BIGRAMS VOCAB BOTH FOR TENSOR AND BIES\n",
    "tensor_bigrams = make_vocab(tensor_result,2)\n",
    "bies_bigrams = make_vocab(bies_result,2)\n",
    "bies_unigrams = make_vocab(bies_result,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrammi:  [(0, 0), ('UNK', 1), ('“', 2), ('人', 3), ('们', 4), ('常', 5), ('说', 6), ('生', 7), ('活', 8), ('是', 9)]\n",
      "lunghezza unigrammi:  5169\n",
      "Bigrammi:  [(0, 0), ('UNK', 1), ('“人', 2), ('人们', 3), ('们常', 4), ('常说', 5), ('说生', 6), ('生活', 7), ('活是', 8), ('是一', 9)]\n",
      "lunghezza bigrammi:  426612\n",
      "3\n",
      "bies unigrammi:  dict_items([(0, 0), ('UNK', 1), ('s', 2), ('b', 3), ('e', 4), ('i', 5)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Unigrammi: \", list(tensor_unigrams.items())[:10])\n",
    "print(\"lunghezza unigrammi: \", len(tensor_unigrams))\n",
    "print(\"Bigrammi: \", list(tensor_bigrams.items())[:10])\n",
    "print(\"lunghezza bigrammi: \", len(tensor_bigrams))\n",
    "print(tensor_bigrams['人们'])\n",
    "print(\"bies unigrammi: \",bies_unigrams.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_vector(sentences,unigram_vocab,bigram_vocab):\n",
    "    '''Method that create the feature vectors to pass in the LSTM input\n",
    "        @param sentences the array of chinese phrases\n",
    "        @param unigram_vocab the vocab used for the chinese unigrams\n",
    "        @param bigram_vocab the vocab used for the chinese bigrams\n",
    "        @return the vector result\n",
    "    '''\n",
    "    features_vector = []\n",
    "    for sentence in sentences:\n",
    "        vec_sentence = []\n",
    "        uni = split_into_ngrams(sentence,1)\n",
    "        bi = split_into_ngrams(sentence,2)\n",
    "        for i in range(len(uni)):\n",
    "            vec_feature = []\n",
    "            vec_feature.append(unigram_vocab[uni[i]])\n",
    "            #insert 0 as bigram of last character position\n",
    "            vec_feature.append(0) if i == (len(uni)-1) else vec_feature.append(bigram_vocab[bi[i]])\n",
    "            #vec_feature.append(bigram_vocab[bi[i]])\n",
    "            vec_sentence.append(vec_feature)\n",
    "        features_vector.append(vec_sentence)\n",
    "    return features_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrammi:  ['“', '人', '们', '常', '说', '生', '活', '是', '一', '部', '教', '科', '书', '，', '而', '血', '与', '火', '的', '战', '争', '更', '是', '不', '可', '多', '得', '的', '教', '科', '书', '，', '她', '确', '实', '是', '名', '副', '其', '实', '的', '‘', '我', '的', '大', '学', '’', '。'] 48\n",
      "bigrammi:  ['“人', '人们', '们常', '常说', '说生', '生活', '活是', '是一', '一部', '部教', '教科', '科书', '书，', '，而', '而血', '血与', '与火', '火的', '的战', '战争', '争更', '更是', '是不', '不可', '可多', '多得', '得的', '的教', '教科', '科书', '书，', '，她', '她确', '确实', '实是', '是名', '名副', '副其', '其实', '实的', '的‘', '‘我', '我的', '的大', '大学', '学’', '’。'] 47\n"
     ]
    }
   ],
   "source": [
    "uni = split_into_ngrams(tensor_result[0],1)\n",
    "bi = split_into_ngrams(tensor_result[0],2)\n",
    "print(\"unigrammi: \",uni,len(uni))\n",
    "print(\"bigrammi: \",bi,len(bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_vector(labels,bies_vocab):\n",
    "    '''Method that create the label vectors to pass as LSTM output\n",
    "        @param labels the array of bies label\n",
    "        @param bies_vocab vocab with unigram label\n",
    "        @return the vector result\n",
    "    '''\n",
    "    labels_vector = []\n",
    "    for lab in labels:\n",
    "        lab_sentence = []\n",
    "        uni = split_into_ngrams(lab,1)\n",
    "        for i in uni:\n",
    "            lab_vec = []\n",
    "            lab_vec.append(bies_vocab[i])\n",
    "            lab_sentence.append(lab_vec)\n",
    "        labels_vector.append(lab_sentence)\n",
    "    return labels_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vector = create_features_vector(tensor_result,tensor_unigrams,tensor_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vector = create_label_vector(bies_result,bies_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prima frase X--> [[327, 15546], [410, 15547], [905, 15548], [1938, 15549], [313, 15550], [1080, 10262], [1605, 10263], [113, 1562], [77, 15551], [701, 3531], [573, 15552], [701, 1879], [227, 15553], [310, 1707], [652, 0]] 15\n",
      "Prima frase Y--> [[3], [5], [4], [3], [4], [3], [4], [3], [4], [3], [5], [5], [4], [3], [4]] 15\n",
      "[]\n",
      "66\n",
      "66\n",
      "sbiebebebiiiesbebebiiiiiiebesbiiiiebiiiebebesbebessbiesbebebebebes\n",
      "由意大利无偿援助三百万美元、国家投资六百多万元人民币修建的西藏急救中心七月十八日开始运行，中心设立的『１２０』急救专线电话同时开通。\n"
     ]
    }
   ],
   "source": [
    "print(\"Prima frase X-->\",features_vector[1023],len(features_vector[1023]))\n",
    "print(\"Prima frase Y-->\",label_vector[1023],len(label_vector[1023]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(features_vector):\n",
    "    '''\n",
    "    Find the value of the max sentence in the features_vector\n",
    "    @param features_vector the vector created for the LSTM input\n",
    "    @return the max length of an element of the features_vector\n",
    "    '''\n",
    "    maxim = 0\n",
    "    i = 0\n",
    "    for sent in features_vector:\n",
    "        if(len(sent)>maxim):\n",
    "            maxim = len(sent)\n",
    "    return maxim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n",
      "581\n"
     ]
    }
   ],
   "source": [
    "#find the max length of the features_vector\n",
    "max_length = find_max_length(features_vector)\n",
    "print(max_length)\n",
    "max_label = find_max_length(label_vector)\n",
    "print(max_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM HERE, I USE NUMPY TO CREATE THE NPARRAYS\n",
    "X = np.array(features_vector)\n",
    "Y = np.array(label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pads_array(vec,max_length):\n",
    "    '''\n",
    "    Add for each sentence a pad_array of zero values until the max_length of a sentence of\n",
    "    feature vector\n",
    "    @param vec the vector that contains the values\n",
    "    @param the max length of an element of the vector\n",
    "    '''\n",
    "    sentences = []\n",
    "    for x in vec:\n",
    "        pad = []\n",
    "        length = len(x)\n",
    "        x = np.pad(x,(0,max_length-length),mode='constant')\n",
    "        pad.append(x)\n",
    "        sentences.append(pad)\n",
    "    return sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
