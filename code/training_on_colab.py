# -*- coding: utf-8 -*-
"""preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uX99DXNvf8CP2ifp-pHH7kIZ9-CfSsZ6

# **PREPROCESS**
"""

import os
from typing import List, Dict
import numpy as np
from tensorflow.keras.utils import to_categorical
import numpy as np

#https://www.dropbox.com/s/zrrxznc52h40ofm/icwb2-data.zip
! wget https://www.dropbox.com/s/zrrxznc52h40ofm/icwb2-data.zip
! unzip icwb2-data.zip

def create_bies_format(line):
    '''
    Method that create the corrispettive bies format of a string
    @param line = the input string
    @return the bias format of the input line
    '''
    i = 0
    bies = ""
    line = line.replace('\n',"")
    line = line.replace('\u3000',' ')
    for index in range(len(line)):
        if(index < len(line)-1):
            if(line[index] == ' '):
                i = 0
            elif(i == 0 and line[index+1] == ' '):
                i = 0
                bies+='S'
            elif(i == 0 and line[index+1] != ' '):
                bies+='B'
                i+=1
            elif(i > 0 and line[index+1] != ' '):
                bies+='I'
                i+=1
            else :
                bies+='E'
                i=0
        else:
            if(i==0):
                bies+='S'
            elif(i>0):
                bies+='E'
    return bies

def create_bies_lines(path):
    '''
    method used to create an array that contains the bies format lines
    of a specific file passed through the path
    @param path = the pathfile of the file to convert
    @return an array with bies lines
    '''
    bies_lines = []
    with open(path,'r',encoding='utf8') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip()
            bies = create_bies_format(line)
            if (len(line) != 0):
              bies_lines.append(bies)
        f.close()
    return bies_lines

def delete_spaces_file(path):
    '''
    Method used to create an array that contains the lines without spaces
    @param path input file
    @return an array with sentences without space
    '''
    result = []
    with open(path,'r',encoding='utf8') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip()
            line = line.replace('\u3000'," ")
            line = line.replace(" ","")
            if(len(line) != 0):
              result.append(line)
        f.close()
    return result

def create_file(path,lines):
    '''
    Method used to create a file either for label and tensorInput 
    @param path = the path where to save the file
    @param bies_lines = the array containing the bies format sentences
    '''
    with open(path,'w') as f:
        for line in lines:
            f.write(line+'\n')
        f.close()

def split_into_ngrams(sentence: str, n : int):
    """
    Split a sentence in array of ngrams
    :param sentence Sentence as str
    :return an array of ngrams
    """
    ngrams = []
    for i in range(len(sentence)):
        ngram = sentence[i:i+n]
        if(n >= 2 and i == len(sentence)-1): ngram = ngram+"<END>"
        ngrams.append(ngram)
    return ngrams
#print(split_into_ngrams(train_tensor_lines[2392],2))

def make_vocab(sentences,n):
    '''
    :param sentences List of sentences used to build the vocab
    :return vocab Dictionary that has as key the ngram and as a value the index
    '''
    vocab = {"<PAD>":0,"UNK": 1}
    for sentence in sentences:
        bigrams = split_into_ngrams(sentence,n)
        for bigram in bigrams:
            if bigram not in vocab:
                vocab[bigram] = len(vocab)
    return vocab

def make_vocab_label():
    vocab = {"B":0,"I":1,"E":2,"S":3}
    return vocab

def word_to_index(sentences,vocab,n):
    '''Method that create the feature vectors to pass in the LSTM input
        @param sentences that we want convert
        @param vocab use to map word to index
        @param n indicates the n-grams split
        @return the vector result
    '''
    features_vector = []
    for sentence in sentences:
        vec_sentence = []
        split = split_into_ngrams(sentence,n)
        for s in split:
          if s not in vocab:
            vec_sentence.append(vocab["UNK"])
          else:
            vec_sentence.append(vocab[s])
        features_vector.append(np.array(vec_sentence))
    return np.array(features_vector)

def find_max_length(vector):
    '''
    Find the value of the max sentence in the features_vector
    @param features_vector the vector created for the LSTM input
    @return the max length of an element of the features_vector
    '''
    maxim = 0
    i = 0
    for el in vector:
        if(len(el)>maxim):
            maxim = len(el)
    return maxim

def add_pads_array(vec,max_length):
    '''
    Add for each sentence a pad_array of zero values until the max_length of a sentence of
    feature vector
    @param vec the vector that contains the values
    @param the max length of an element of the vector
    '''
    sentences = []
    for x in vec:
        length = len(x)
        a = np.pad(x,(0,max_length-length),mode='constant')
        sentences.append(a)
    return sentences

def convert_to_categorical(vec):
    matrix = []
    for v in vec:
        #rint(v)
        a = to_categorical(v,num_classes=4)
        matrix.append(np.array(a))
    return np.array(matrix)

"""# **MODEL**"""

from keras.layers import Input, Embedding, LSTM, Dense, concatenate,Bidirectional, Dropout, TimeDistributed
from keras.models import Model
from keras import optimizers
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
import math
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

#"../resources/icwb2-data/training/msr_training.utf8"
#DEFINISCO I PATH DEI FILES TRAINING
folder_train = "icwb2-data/training/"
#MSR
msr_path_training_file = folder_train+"msr_training.utf8"
msr_path_training_label = folder_train+"bies_msr_training.utf8"
msr_path_training_tensor = folder_train+"tensor_msr_training.utf8"
#CITYU

cityu_path_training_file = folder_train+"cityu_training_simply.utf8"
cityu_path_training_label = folder_train+"bies_cityu_training_simply.utf8"
cityu_path_training_tensor = folder_train+"tensor_cityu_training_simply.utf8"
#AS
as_path_training_file = folder_train+"as_training_simply.utf8"
as_path_training_label = folder_train+"bies_as_training_simply.utf8"
as_path_training_tensor = folder_train+"tensor_as_training_simply.utf8"
#PKU
pku_path_training_file = folder_train+"pku_training.utf8"
pku_path_training_label = folder_train+"bies_pku_training.utf8"
pku_path_training_tensor = folder_train+"tensor_pku_training.utf8"

#DEFINISCO I PATH DEI FILES GOLD
folder_dev = "icwb2-data/gold/"
#MSR
msr_path_dev_file = folder_dev+"msr_test_gold.utf8"
msr_path_dev_label = folder_dev+"bies_msr_test_gold.utf8"
msr_path_dev_tensor = folder_dev+"tensor_msr_test_gold.utf8"
#CITYU

cityu_path_dev_file = folder_dev+"cityu_test_gold_simply.utf8"
cityu_path_dev_label = folder_dev+"bies_cityu_test_gold_simply.utf8"
cityu_path_dev_tensor = folder_dev+"tensor_cityu_test_gold_simply.utf8"
#AS

as_path_dev_file = folder_dev+"as_testing_gold_simply.utf8"
as_path_dev_label = folder_dev+"bies_as_testing_gold_simply.utf8"
as_path_dev_tensor = folder_dev+"tensor_as_testing_gold_simply.utf8"
#PKU
pku_path_dev_file = folder_dev+"pku_test_gold.utf8"
pku_path_dev_label = folder_dev+"bies_pku_test_gold.utf8"
pku_path_dev_tensor = folder_dev+"tensor_pku_test_gold.utf8"


#CREO GLI ARRAY DA INSERIRE NEI FILES DA CREARE PER IL TRAINING
#msr
msr_train_bies_lines = create_bies_lines(msr_path_training_file)
msr_train_tensor_lines = delete_spaces_file(msr_path_training_file)
#as

as_train_bies_lines = create_bies_lines(as_path_training_file)
as_train_tensor_lines = delete_spaces_file(as_path_training_file)
#cityu

cityu_train_bies_lines = create_bies_lines(cityu_path_training_file)
cityu_train_tensor_lines = delete_spaces_file(cityu_path_training_file)
#pku
pku_train_bies_lines = create_bies_lines(pku_path_training_file)
pku_train_tensor_lines = delete_spaces_file(pku_path_training_file)

#CREO GLI ARRAY DA INSERIRE NEI FILES DA CREARE PER IL DEV
#msr
msr_dev_bies_lines = create_bies_lines(msr_path_dev_file)
msr_dev_tensor_lines = delete_spaces_file(msr_path_dev_file)
#as

as_dev_bies_lines = create_bies_lines(as_path_dev_file)
as_dev_tensor_lines = delete_spaces_file(as_path_dev_file)
#cityu

cityu_dev_bies_lines = create_bies_lines(cityu_path_dev_file)
cityu_dev_tensor_lines = delete_spaces_file(cityu_path_dev_file)
#pku
pku_dev_bies_lines = create_bies_lines(pku_path_dev_file)
pku_dev_tensor_lines = delete_spaces_file(pku_path_dev_file)

#CREO I RISPETTIVI FILES DI TRAINING
#msr
create_file(msr_path_training_label,msr_train_bies_lines)
create_file(msr_path_training_tensor,msr_train_tensor_lines)
#as

create_file(as_path_training_label,as_train_bies_lines)
create_file(as_path_training_tensor,as_train_tensor_lines)

#cityu

create_file(cityu_path_training_label,cityu_train_bies_lines)
create_file(cityu_path_training_tensor,cityu_train_tensor_lines)
#pku
create_file(pku_path_training_label,pku_train_bies_lines)
create_file(pku_path_training_tensor,pku_train_tensor_lines)

#CREO FILE DEV
#msr
create_file(msr_path_dev_label,msr_dev_bies_lines)
create_file(msr_path_dev_tensor,msr_dev_tensor_lines)
#as

create_file(as_path_dev_label,as_dev_bies_lines)
create_file(as_path_dev_tensor,as_dev_tensor_lines)

#cityu
create_file(cityu_path_dev_label,cityu_dev_bies_lines)
create_file(cityu_path_dev_tensor,cityu_dev_tensor_lines)
#pku
create_file(pku_path_dev_label,pku_dev_bies_lines)
create_file(pku_path_dev_tensor,pku_dev_tensor_lines)

concat_train_tensor = as_train_tensor_lines + msr_train_tensor_lines + pku_train_tensor_lines + cityu_train_tensor_lines
concat_train_bies = as_train_bies_lines + msr_train_bies_lines + pku_train_bies_lines + cityu_train_bies_lines
concat_dev_tensor = as_dev_tensor_lines + msr_dev_tensor_lines + pku_dev_tensor_lines + cityu_dev_tensor_lines
concat_dev_bies = as_dev_bies_lines + msr_dev_bies_lines + pku_dev_bies_lines + cityu_dev_bies_lines
#CREO FILE CONCAT
create_file(folder_train+"tensor_concat_train.utf8",concat_train_tensor)
create_file(folder_train+"bies_concat_train.utf8",concat_train_bies)

create_file(folder_dev+"tensor_concat_gold.utf8",concat_dev_tensor)
create_file(folder_dev+"bies_concat_gold.utf8",concat_dev_bies)

#CREATE THE UNIGRAMS VOCAB FOR TENSOR 
vocab_unigrams = make_vocab(concat_train_tensor,1)
#CREATE THE BIGRAMS VOCAB BOTH FOR TENSOR AND BIES
vocab_bigrams = make_vocab(concat_train_tensor,2)
vocab_labels = make_vocab_label()
len(vocab_bigrams)

#create word to index of train_x train_y 
train_x_uni = word_to_index(msr_train_tensor_lines,vocab_unigrams,1)
train_x_bi = word_to_index(msr_train_tensor_lines,vocab_bigrams,2)
train_y_uni = word_to_index(msr_train_bies_lines,vocab_labels,1)

#create word to index of dev_x dev_y
dev_x_uni = word_to_index(msr_dev_tensor_lines,vocab_unigrams,1)
dev_x_bi = word_to_index (msr_dev_tensor_lines,vocab_bigrams,2)
dev_y_uni = word_to_index(msr_dev_bies_lines,vocab_labels,1)

print(train_x_uni)
max_length_sample = find_max_length(train_x_uni)
train_y_uni = convert_to_categorical(train_y_uni)
dev_y_uni = convert_to_categorical(dev_y_uni)

def create_keras_model_parallel(vocab_size_uni,vocab_size_bi, char_embedding_size,
                                bigram_embedding_size, hidden_size,input_dropout,lstm_dropout):
    print("Creating KERAS model")
    
    input_uni = Input(shape=(None,))
    input_bi = Input(shape=(None,))
    
    
    x1 = Embedding(vocab_size_uni,char_embedding_size,mask_zero = True)(input_uni)
    x2 = Embedding(vocab_size_bi,bigram_embedding_size,mask_zero = True)(input_bi)
    
    drp1 = Dropout(input_dropout)(x1)
    drp2 = Dropout(input_dropout)(x2)
    
    concat_input = concatenate([drp1,drp2],name="concat")
    
    
    bidi = Bidirectional(LSTM(hidden_size,name="back_lstm",return_sequences=True,recurrent_dropout=lstm_dropout,dropout=lstm_dropout))(concat_input)
    
    output = TimeDistributed(Dense(4,activation='softmax'))(bidi)
    model = Model(inputs=[input_uni,input_bi],outputs=output)
    return model

def compile_keras_model(model,optimizer,learning_rate):
    if(str.lower(optimizer) == "sgd"):
      opt = optimizers.SGD(lr=learning_rate, clipnorm=0.1 , momentum=0.95, nesterov=True)
    elif(str.lower(optimizer) == "adam"):
      opt = optimizers.Adam(lr=learning_rate,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])
    return model

def batch_generator(x_uni, x_bi, Y, batch_size, shuffle=False):
    while True:
      if not shuffle:
          for start in range(0, len(x_uni), batch_size):
              end = start + batch_size
              max_length = find_max_length(x_uni[start:end])
              a = x_uni[start:end]
              b = x_bi[start:end]
              c = Y[start:end]
              a = pad_sequences(a,maxlen=max_length,padding='post')
              b = pad_sequences(b,maxlen=max_length,padding='post')
              c = pad_sequences(c,maxlen=max_length,padding='post')
              #c = convert_to_categorical(c)
              yield [a,b], c
      else:
          perm = np.random.permutation(len(x_uni))
          for start in range(0, len(x_uni), batch_size):
              end = start + batch_size
              max_length = find_max_length(x_uni[perm[start:end]])
              a = x_uni[perm[start:end]]
              b = x_bi[perm[start:end]]
              c = Y[perm[start:end]]
              a = pad_sequences(a,maxlen=max_length,padding='post')
              b = pad_sequences(b,maxlen=max_length,padding='post')
              c = pad_sequences(c,maxlen=max_length,padding='post')
              #c = convert_to_categorical(c)
              yield [a,b], c

#DEFINE SOME COSTANTS
MAX_LENGTH = max_length_sample
VOCAB_SIZE_UNI = len(vocab_unigrams)
VOCAB_SIZE_BI = len(vocab_bigrams)
CHAR_EMBEDDING_SIZE = 32
BIGRAM_EMBEDDING_SIZE = [16, 32, 64]
LEARNING_RATE = [0.04, 0.035, 0.03, 0.002,0.0099]
HIDDEN_SIZE = 256
INPUT_DROPOUT = [0,0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6]
LSTM_DROPOUT = [0,0.1, 0.2, 0.3, 0.4,0.5]

model = create_keras_model_parallel(VOCAB_SIZE_UNI,VOCAB_SIZE_BI,CHAR_EMBEDDING_SIZE,BIGRAM_EMBEDDING_SIZE[1],
                                     HIDDEN_SIZE,INPUT_DROPOUT[1],LSTM_DROPOUT[2])
 model = compile_keras_model(model,"adam",LEARNING_RATE[2])
print(model.summary())

#MODE WITH BATCH GENERATORS
batch_size = 128
epochs = 15
#cbk = K.callbacks.TensorBoard("logging/keras_model")
early_stopping = EarlyStopping(monitor="val_loss",patience=3)
checkpointer = ModelCheckpoint(filepath="weights.hdf5", monitor='val_acc', verbose=1, save_best_only=True,mode='max')
cbk = [early_stopping,checkpointer]
print("\nStarting training...")
#model.fit([train_x_uni,train_x_bi], train_y_uni, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=([dev_x_uni,dev_x_bi], dev_y_uni)) 
stats = model.fit_generator(batch_generator(train_x_uni,train_x_bi,train_y_uni,batch_size,shuffle=False),
                    steps_per_epoch=70,
                    epochs=epochs,
                    callbacks=cbk,
                    verbose = 1,
                    validation_data=batch_generator(dev_x_uni,dev_x_bi,dev_y_uni,batch_size,shuffle=False),
                    validation_steps=70)
print("Training complete.\n")
#int(len(train_x_uni)/batch_size)

#int(len(dev_x_uni)/batch_size))

#Save the model and the weights
model.save("model_keras_1.h5")

model.load_weights("weights.hdf5")

#SHOW THE STATISTICS ABOUT THE MODEL
# summarize history for accuracy
plt.plot(stats.history['acc'])
plt.plot(stats.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(stats.history['loss'])
plt.plot(stats.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()